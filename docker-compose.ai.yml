version: '3.8'

services:
  # ============================================================================
  # LLAMA 3.1 8B - Article Writing & Headlines
  # ============================================================================
  ollama-llama:
    image: ollama/ollama:latest
    container_name: coindaily-ai-llama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-llama-models:/root/.ollama
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
      - GGML_AVX2=1
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network

  # ============================================================================
  # DEEPSEEK-R1 8B - Review Agent with Reasoning
  # ============================================================================
  ollama-deepseek:
    image: ollama/ollama:latest
    container_name: coindaily-ai-deepseek
    restart: unless-stopped
    ports:
      - "11435:11434"
    volumes:
      - ollama-deepseek-models:/root/.ollama
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
      - GGML_AVX2=1
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network

  # ============================================================================
  # NLLB-200 DISTILLED 600M - Translation Service
  # ============================================================================
  nllb-translation:
    build:
      context: ./docker/nllb
      dockerfile: Dockerfile
    image: coindaily-nllb:latest
    container_name: coindaily-ai-translation
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - nllb-models:/models
      - nllb-cache:/root/.cache/huggingface
    environment:
      - MODEL_NAME=facebook/nllb-200-distilled-600M
      - BATCH_SIZE=8
      - MAX_LENGTH=512
      - NUM_BEAMS=4
      - DEVICE=cpu
      - WORKERS=4
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network

  # ============================================================================
  # SDXL with OpenVINO - Image Generation
  # ============================================================================
  sdxl-openvino:
    build:
      context: ./docker/sdxl
      dockerfile: Dockerfile
    image: coindaily-sdxl:latest
    container_name: coindaily-ai-image
    restart: unless-stopped
    ports:
      - "7860:7860"
    volumes:
      - sdxl-models:/app/models
      - sdxl-outputs:/app/outputs
    environment:
      - COMMANDLINE_ARGS=--api --xformers --opt-sdp-attention --no-half --precision full --listen --port 7860
      - PYTORCH_OPENVINO_DEVICE=CPU
      - STABLE_DIFFUSION_MODEL=stabilityai/stable-diffusion-xl-base-1.0
      - NEGATIVE_PROMPT_DEFAULT=blurry, low quality, distorted, watermark, text overlay, signature, username, low resolution, bad anatomy, poorly drawn
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 8G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/sdapi/v1/sd-models"]
      interval: 60s
      timeout: 30s
      retries: 3
    networks:
      - ai-network

  # ============================================================================
  # BGE Embeddings - RAG Pipeline
  # ============================================================================
  bge-embeddings:
    build:
      context: ./docker/embeddings
      dockerfile: Dockerfile
    image: coindaily-embeddings:latest
    container_name: coindaily-ai-embeddings
    restart: unless-stopped
    ports:
      - "8081:8081"
    volumes:
      - embeddings-models:/models
      - embeddings-cache:/root/.cache/huggingface
    environment:
      - MODEL_NAME=BAAI/bge-small-en-v1.5
      - BATCH_SIZE=32
      - MAX_LENGTH=512
      - DEVICE=cpu
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai-network

  # ============================================================================
  # Monitoring - Prometheus
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: coindaily-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - ai-network

  # ============================================================================
  # Monitoring - Grafana
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: coindaily-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus
    networks:
      - ai-network

  # ============================================================================
  # Redis - Queue Management
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: coindaily-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - ai-network

# ============================================================================
# Networks
# ============================================================================
networks:
  ai-network:
    driver: bridge

# ============================================================================
# Volumes
# ============================================================================
volumes:
  ollama-llama-models:
    driver: local
  ollama-deepseek-models:
    driver: local
  nllb-models:
    driver: local
  nllb-cache:
    driver: local
  sdxl-models:
    driver: local
  sdxl-outputs:
    driver: local
  embeddings-models:
    driver: local
  embeddings-cache:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  redis-data:
    driver: local
